{
    "cells": [{
        "cell_type":
        "code",
        "execution_count":
        1,
        "metadata": {},
        "outputs": [],
        "source": [
            "from sklearn.metrics.pairwise import pairwise_distances\n",
            "import tensorflow as tf\n", "import numpy as np\n",
            "import time\n", "import h5py\n", "import sys"
        ]
    }, {
        "cell_type":
        "code",
        "execution_count":
        3,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Resnet blocks \n",
            "def identity_block(input_tensor, kernel_size, filters, stage, block, is_training):\n",
            "    with tf.variable_scope(\"stage-{}_block-{}\".format(stage, block), reuse=tf.AUTO_REUSE) as scope:\n",
            "        x = tf.layers.batch_normalization(input_tensor, training=is_training)\n",
            "        x = tf.nn.relu(x)\n",
            "        x = tf.layers.conv2d(x, filters[0], kernel_size[0], padding=\"same\")\n",
            "\n",
            "        x = tf.layers.batch_normalization(x, training=is_training)\n",
            "        x = tf.nn.relu(x)\n",
            "        x = tf.layers.conv2d(x, filters[1], kernel_size[1], padding=\"same\")\n",
            "\n", "        x = tf.concat([x, input_tensor], axis=-1)\n",
            "    return x\n", "\n",
            "def conv_block(input_tensor, kernel_size, filters, stage, block, is_training, strides=2):\n",
            "    with tf.variable_scope(\"stage-{}_block-{}\".format(stage, block), reuse=tf.AUTO_REUSE) as scope:\n",
            "        x = tf.layers.batch_normalization(input_tensor, training=is_training)\n",
            "        x = tf.nn.relu(x)\n",
            "        x = tf.layers.conv2d(x, filters[0], kernel_size[0], strides=strides, padding=\"same\")\n",
            "\n",
            "        x = tf.layers.batch_normalization(x, training=is_training)\n",
            "        x = tf.nn.relu(x)\n",
            "        x = tf.layers.conv2d(x, filters[1], kernel_size[1], padding=\"same\")\n",
            "\n",
            "        shortcut = tf.layers.batch_normalization(input_tensor, training=is_training)\n",
            "        shortcut = tf.nn.relu(shortcut)\n",
            "        shortcut = tf.layers.conv2d(shortcut, filters[1], kernel_size[1], strides=strides, padding=\"same\")\n",
            "\n", "        x = tf.concat([x, shortcut], axis=-1)\n",
            "    return x\n", "\n", "# slimmed downed resnet\n",
            "def classifier(inputs, num_classes, num_embeddings, is_training):\n",
            "    with tf.variable_scope(\"stage-1_block-a\", reuse=tf.AUTO_REUSE) as scope:\n",
            "        x = tf.layers.conv2d(inputs, 64, 7, strides=2, padding=\"same\")\n",
            "        x = tf.layers.batch_normalization(x, training=is_training)\n",
            "        x = tf.nn.relu(x)\n",
            "        x = tf.layers.max_pooling2d(x, 3, strides=2, padding=\"same\")\n",
            "\n",
            "    x = conv_block    (x, [1, 3], [64, 256], stage=2, block='a', is_training=is_training)\n",
            "    x = identity_block(x, [1, 3], [64, 256], stage=2, block='b', is_training=is_training)\n",
            "\n",
            "    x = conv_block    (x, [1, 3], [128, 512], stage=3, block='a', is_training=is_training)\n",
            "    x = identity_block(x, [1, 3], [128, 512], stage=3, block='b', is_training=is_training)\n",
            "\n",
            "    x = conv_block    (x, [1, 3], [256, 1024], stage=4, block='a', is_training=is_training)\n",
            "    x = identity_block(x, [1, 3], [256, 1024], stage=4, block='b', is_training=is_training)\n",
            "\n",
            "    x = conv_block    (x, [1, 3], [512, 2048], stage=5, block='a', is_training=is_training)\n",
            "    x = identity_block(x, [1, 3], [512, 2048], stage=5, block='b', is_training=is_training)\n",
            "\n",
            "    with tf.variable_scope(\"embeddings\", reuse=tf.AUTO_REUSE) as scope:\n",
            "        x = tf.layers.average_pooling2d(x, (x.get_shape()[-3], x.get_shape()[-2]), 1) #global average pooling\n",
            "        x = tf.layers.flatten(x)\n",
            "        embeddings = tf.layers.dense(x, num_embeddings, activation=None)\n",
            "        \n",
            "    with tf.variable_scope(\"logits\", reuse=tf.AUTO_REUSE) as scope:\n",
            "        pred_logits = tf.nn.relu(embeddings)\n",
            "        pred_logits = tf.layers.dense(pred_logits, num_classes, activation=None)\n",
            "\n", "    return embeddings, pred_logits"
        ]
    }, {
        "cell_type":
        "code",
        "execution_count":
        4,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Center Loss (Metric Learning)\n",
            "# https://github.com/EncodeTS/TensorFlow_Center_Loss/blob/master/center_loss.py\n",
            "def get_center_loss(features, labels, alpha, num_classes):\n",
            "    \n", "    len_features = features.get_shape()[1] \n",
            "    with tf.variable_scope(\"central_loss\", reuse=tf.AUTO_REUSE) as scope:\n",
            "        centers = tf.get_variable('centers', [num_classes, len_features], dtype=tf.float32,\n",
            "            initializer=tf.constant_initializer(0), trainable=False)\n",
            "        \n", "    labels = tf.reshape(labels, [-1])\n", "\n",
            "    centers_batch = tf.gather(centers, labels)\n",
            "    loss = tf.nn.l2_loss(features - centers_batch)\n", "\n",
            "    unique_label, unique_idx, unique_count = tf.unique_with_counts(labels)\n",
            "    appear_times = tf.gather(unique_count, unique_idx)\n",
            "    appear_times = tf.reshape(appear_times, [-1, 1])\n", "\n",
            "    diff = centers_batch - features\n",
            "    diff = diff / tf.cast((1 + appear_times), tf.float32)\n",
            "    diff = alpha * diff\n", "\n",
            "    centers_update_op = tf.scatter_sub(centers, labels, diff)\n",
            "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, centers_update_op)\n",
            "\n", "    return loss, centers"
        ]
    }, {
        "cell_type":
        "code",
        "execution_count":
        5,
        "metadata": {},
        "outputs": [],
        "source": [
            "def unison_shuffled_copies(a, b):\n",
            "    p = np.random.permutation(a.shape[0])\n",
            "    return a[p], b[p]\n", "\n",
            "def get_batch(data_x, data_y, batch_size):\n",
            "    while True:\n",
            "        data_x, data_y = unison_shuffled_copies(data_x, data_y)\n",
            "        for index in range(0, data_x.shape[0], batch_size):\n",
            "            x, y = data_x[index:index+batch_size], data_y[index:index+batch_size]\n",
            "            if x.shape[0] == batch_size:\n",
            "                yield x, y"
        ]
    }, {
        "cell_type":
        "code",
        "execution_count":
        22,
        "metadata": {},
        "outputs": [{
            "name":
            "stdout",
            "output_type":
            "stream",
            "text": [
                "(1359, 2000, 270, 1) (1359, 50) (240, 2000, 270, 1) (240, 50) (400, 2000, 270, 1) (400,) \n",
                "\n"
            ]
        }],
        "source": [
            "# define model parameters and load dataset\n",
            "data_file =    \"data/CSI_50_500.h5\"\n",
            "weights_path = \"data/weights/central_loss_50_500/central_loss_model.ckpt\"\n",
            "\n", "num_embeddings = 64\n", "num_classes = 50\n",
            "batch_size = 8\n", "ratio = 0.848\n", "alpha = 0.5\n", "    \n",
            "hf = h5py.File(data_file, 'r')\n",
            "X_train = np.expand_dims(hf.get('X_train'), axis=-1)\n",
            "X_test = np.expand_dims(hf.get('X_test'), axis=-1)\n",
            "y_train = np.eye(num_classes)[hf.get('y_train')]\n",
            "y_test = np.eye(num_classes)[hf.get('y_test')]\n",
            "y_train_n = np.array(hf.get('y_train'))\n",
            "y_test_n = np.array(hf.get('y_test'))\n",
            "train_classes = np.array(hf.get('labels')).astype(str)\n",
            "X_data = np.expand_dims(hf.get('X_intruder'), axis=-1)\n",
            "y_data = np.array(hf.get('y_intruder'))\n", "hf.close()\n", "\n",
            "rows, cols, channels = X_train.shape[1:]\n", "\n",
            "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, X_data.shape, y_data.shape, \"\\n\")"
        ]
    }, {
        "cell_type":
        "code",
        "execution_count":
        23,
        "metadata": {},
        "outputs": [],
        "source": [
            "# method to get model predictions\n",
            "tf.reset_default_graph()\n", "\n", "with tf.device('/cpu:0'):\n",
            "    with tf.variable_scope(\"Inputs\") as scope:\n",
            "        X = tf.placeholder(tf.float32, [None, rows, cols, channels])\n",
            "        Y = tf.placeholder(tf.float32, [None, num_classes])\n",
            "        \n", "    is_training = tf.placeholder(tf.bool)\n", "\n",
            "    # Calculate the gradients for each model tower.\n",
            "    with tf.device(\"/gpu:0\"):\n",
            "        with tf.name_scope(\"resnet_0\") as scope:\n",
            "            embeddings, pred_logits = classifier(X, num_classes, num_embeddings, is_training)\n",
            "\n",
            "            center_loss, centers = get_center_loss(embeddings, tf.argmax(Y, axis=1), alpha, num_classes)\n",
            "    # Initializing the variables\n",
            "    init = tf.global_variables_initializer()\n",
            "    merged = tf.summary.merge_all()\n",
            "    saver = tf.train.Saver(max_to_keep=5)"
        ]
    }, {
        "cell_type":
        "code",
        "execution_count":
        24,
        "metadata": {},
        "outputs": [{
            "name":
            "stdout",
            "output_type":
            "stream",
            "text": [
                "INFO:tensorflow:Restoring parameters from /home/kjakkala/neuralwave/data/weights/central_loss_mc/mc_0/central_loss_model.ckpt-10500\n",
                "(400, 50) (240, 50) (1359, 50) (400, 64) (240, 64) (1359, 64) (50, 64)\n"
            ]
        }],
        "source": [
            "# get model predictions\n", "X_data_embeddings = []\n",
            "X_test_embeddings = []\n", "X_data_logits = []\n",
            "X_test_logits = []\n", "X_train_embeddings = []\n",
            "X_train_logits = []\n", "final_centers = []\n", "\n",
            "with tf.Session(config=tf.ConfigProto(allow_soft_placement = True)) as sess:\n",
            "    sess.run(init)\n",
            "    saver.restore(sess, tf.train.latest_checkpoint(weights_path))\n",
            "    \n",
            "    final_centers = sess.run([centers], feed_dict={is_training: False})\n",
            "\n", "    for step in range(X_data.shape[0]):\n",
            "        tmp1, tmp2 = sess.run([embeddings, pred_logits], feed_dict={X: np.expand_dims(X_data[step], axis=0), is_training: False})\n",
            "        X_data_embeddings.append(tmp1)\n",
            "        X_data_logits.append(tmp2)\n", "\n",
            "    for step in range(X_test.shape[0]):\n",
            "        tmp1, tmp2 = sess.run([embeddings, pred_logits], feed_dict={X: np.expand_dims(X_test[step], axis=0), is_training: False})\n",
            "        X_test_embeddings.append(tmp1)\n",
            "        X_test_logits.append(tmp2)\n", "        \n",
            "    for step in range(X_train.shape[0]):\n",
            "        tmp1, tmp2 = sess.run([embeddings, pred_logits], feed_dict={X: np.expand_dims(X_train[step], axis=0), is_training: False})\n",
            "        X_train_embeddings.append(tmp1)\n",
            "        X_train_logits.append(tmp2)\n", "\n",
            "X_test_embeddings = np.squeeze(X_test_embeddings)\n",
            "X_data_embeddings = np.squeeze(X_data_embeddings)\n",
            "X_train_embeddings = np.squeeze(X_train_embeddings)\n",
            "X_train_logits = np.squeeze(X_train_logits)\n",
            "X_test_logits = np.squeeze(X_test_logits)\n",
            "X_data_logits = np.squeeze(X_data_logits)\n",
            "final_centers = np.squeeze(final_centers)\n", "\n",
            "print(X_data_logits.shape, X_test_logits.shape, X_train_logits.shape, X_data_embeddings.shape, X_test_embeddings.shape, X_train_embeddings.shape, final_centers.shape)"
        ]
    }, {
        "cell_type":
        "code",
        "execution_count":
        25,
        "metadata": {},
        "outputs": [{
            "name": "stdout",
            "output_type": "stream",
            "text": ["0.03899926416482708\n", "0.020833333333333332\n"]
        }],
        "source": [
            "print(np.mean(np.equal(np.argmax(X_train_logits, axis=-1), np.argmax(y_train, axis=-1))))\n",
            "print(np.mean(np.equal(np.argmax(X_test_logits, axis=-1), np.argmax(y_test, axis=-1))))"
        ]
    }, {
        "cell_type":
        "code",
        "execution_count":
        32,
        "metadata": {},
        "outputs": [{
            "name": "stdout",
            "output_type": "stream",
            "text": ["100.0\n", "90.0\n", "73.0\n"]
        }],
        "source": [
            "# predicts intruders or classes not present in the training dataset by thresholding the data embeddings\n",
            "def get_preds(data):\n", "    class_thresholds = []\n",
            "    for i in range(num_classes):\n",
            "        class_thresholds.append(np.max(np.min(pairwise_distances(X_train_embeddings[np.where(y_train_n == i)], final_centers), axis=1)))\n",
            "    class_thresholds = np.array(class_thresholds)\n",
            "    class_thresholds += class_thresholds*2.5\n", "\n",
            "    distance = np.min(pairwise_distances(data, final_centers), axis=1)\n",
            "    class_preds = np.argmin(pairwise_distances(data, final_centers), axis=1)\n",
            "\n", "    for i in range(data.shape[0]):\n",
            "        if (distance[i] > class_thresholds[class_preds[i]]):\n",
            "            class_preds[i] = -1\n", "            \n",
            "    return class_preds, distance\n", "\n",
            "print(np.mean(np.equal(get_preds(X_train_embeddings)[0], y_train_n))*100)\n",
            "print(np.mean(np.equal(get_preds(X_test_embeddings)[0], y_test_n))*100)\n",
            "print(np.mean(np.equal(get_preds(X_data_embeddings)[0], -1*np.ones(X_data_embeddings.shape[0])))*100)"
        ]
    }, {
        "cell_type":
        "code",
        "execution_count":
        22,
        "metadata": {},
        "outputs": [{
            "name":
            "stdout",
            "output_type":
            "stream",
            "text": [
                "wei  \tbenjamin_poole                \n",
                "ding \tbenjamin_poole                \n",
                "wei  \tbenjamin_poole                \n",
                "wei  \tbenjamin_poole                \n",
                "wei  \tbenjamin_poole                \n",
                "mark_powers\tbenjamin_poole                \n",
                "wei  \tbenjamin_poole                \n",
                "wei  \tbenjamin_poole                \n",
                "wei  \tbenjamin_poole                \n",
                "ding \tbenjamin_poole                \n",
                "wei  \tbenjamin_poole                \n",
                "ding \tbenjamin_poole                \n",
                "wei  \tbenjamin_poole                \n",
                "ding \tbenjamin_poole                \n",
                "wei  \tbenjamin_poole                \n",
                "ding \tbenjamin_poole                \n",
                "wei  \tbenjamin_poole                \n",
                "wei  \tbenjamin_poole                \n",
                "ding \tbenjamin_poole                \n",
                "wei  \tbenjamin_poole                \n",
                "mark_powers\tbenjamin_poole                \n",
                "wei  \tbenjamin_poole                \n",
                "wei  \tbenjamin_poole                \n",
                "wei  \tbenjamin_poole                \n",
                "wei  \tbenjamin_poole                \n",
                "ding \tbenjamin_poole                \n",
                "wei  \tbenjamin_poole                \n",
                "wei  \tbenjamin_poole                \n",
                "wei  \tbenjamin_poole                \n",
                "ding \tbenjamin_poole                \n",
                "ding \tbenjamin_poole                \n",
                "wei  \tbenjamin_poole                \n",
                "wei  \tbenjamin_poole                \n",
                "ding \tbenjamin_poole                \n",
                "wei  \tbenjamin_poole                \n",
                "wei  \tbenjamin_poole                \n",
                "wei  \tbenjamin_poole                \n",
                "wei  \tbenjamin_poole                \n",
                "wei  \tbenjamin_poole                \n",
                "wei  \tbenjamin_poole                \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tgokul_rajakumar               \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "ding \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "mark_powers\tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "sanchita_nipunage\tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "mark_powers\tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "wei  \tjasmine_corea                 \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "ding \tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "wei  \tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "sneha\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "wei  \tkarena_huang                  \n",
                "wei  \tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "kailey_wolfe\tkarena_huang                  \n",
                "wahida\tnathan                        \n",
                "wahida\tnathan                        \n",
                "wei  \tnathan                        \n",
                "wahida\tnathan                        \n",
                "wahida\tnathan                        \n",
                "wahida\tnathan                        \n",
                "wahida\tnathan                        \n",
                "wahida\tnathan                        \n",
                "wahida\tnathan                        \n",
                "wei  \tnathan                        \n",
                "wahida\tnathan                        \n",
                "wei  \tnathan                        \n",
                "wahida\tnathan                        \n",
                "wahida\tnathan                        \n",
                "wahida\tnathan                        \n",
                "wahida\tnathan                        \n",
                "wahida\tnathan                        \n",
                "wahida\tnathan                        \n",
                "wahida\tnathan                        \n",
                "wahida\tnathan                        \n",
                "wei  \tnathan                        \n",
                "wahida\tnathan                        \n",
                "wahida\tnathan                        \n",
                "wei  \tnathan                        \n",
                "wahida\tnathan                        \n",
                "wahida\tnathan                        \n",
                "wahida\tnathan                        \n",
                "wahida\tnathan                        \n",
                "wahida\tnathan                        \n",
                "wahida\tnathan                        \n",
                "wahida\tnathan                        \n",
                "wahida\tnathan                        \n",
                "wahida\tnathan                        \n",
                "wahida\tnathan                        \n",
                "wahida\tnathan                        \n",
                "wahida\tnathan                        \n",
                "wei  \tnathan                        \n",
                "wahida\tnathan                        \n",
                "wahida\tnathan                        \n",
                "wahida\tnathan                        \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "carson\tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tpeter_kim                     \n",
                "wei  \tsafat                         \n",
                "sam  \tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "wei  \tsafat                         \n",
                "sam  \tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "sam  \tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "wei  \tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "carson\tsafat                         \n",
                "sam  \tsafat                         \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "mark_powers\tsiqi                          \n",
                "akarsh\tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "trinadh\tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "akarsh\tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "wei  \tsiqi                          \n",
                "cheng\tting                          \n",
                "cheng\tting                          \n",
                "cheng\tting                          \n",
                "akarsh\tting                          \n",
                "cheng\tting                          \n",
                "akarsh\tting                          \n",
                "akarsh\tting                          \n",
                "cheng\tting                          \n",
                "cheng\tting                          \n",
                "cheng\tting                          \n",
                "mark_powers\tting                          \n",
                "cheng\tting                          \n",
                "cheng\tting                          \n",
                "cheng\tting                          \n",
                "akarsh\tting                          \n",
                "akarsh\tting                          \n",
                "akarsh\tting                          \n",
                "akarsh\tting                          \n",
                "akarsh\tting                          \n",
                "wei  \tting                          \n",
                "cheng\tting                          \n",
                "cheng\tting                          \n",
                "wei  \tting                          \n",
                "cheng\tting                          \n",
                "cheng\tting                          \n",
                "akarsh\tting                          \n",
                "wei  \tting                          \n",
                "cheng\tting                          \n",
                "cheng\tting                          \n",
                "cheng\tting                          \n",
                "cheng\tting                          \n",
                "cheng\tting                          \n",
                "cheng\tting                          \n",
                "wei  \tting                          \n",
                "cheng\tting                          \n",
                "cheng\tting                          \n",
                "wei  \tting                          \n",
                "cheng\tting                          \n",
                "akarsh\tting                          \n",
                "cheng\tting                          \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "mark_powers\tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "mark_powers\tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "mark_powers\tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "mark_powers\tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n",
                "wei  \tyutian_gui                    \n"
            ]
        }],
        "source": [
            "intruders = get_preds(X_data_embeddings)[0]\n",
            "for i in range(len(intruders)):\n",
            "    print(\"{: <5}\\t{: <30}\".format(train_classes[intruders[i]], data_classes[y_data[i]]))"
        ]
    }],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.2"
        }
    },
    "nbformat":
    4,
    "nbformat_minor":
    4
}
