{
    "cells": [{
        "cell_type":
        "code",
        "execution_count":
        1,
        "metadata": {},
        "outputs": [],
        "source": [
            "import tensorflow as tf\n", "import numpy as np\n",
            "import h5py\n", "import sys"
        ]
    }, {
        "cell_type":
        "code",
        "execution_count":
        2,
        "metadata": {},
        "outputs": [],
        "source": [
            "# average gradinets from each gpu when using multiple gpus\n",
            "def average_gradients(tower_grads):\n",
            "    average_grads = []\n",
            "    for grad_and_vars in zip(*tower_grads):\n",
            "        # Note that each grad_and_vars looks like the following:\n",
            "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
            "        grads = []\n", "        for g, _ in grad_and_vars:\n",
            "            # Add 0 dimension to the gradients to represent the tower.\n",
            "            expanded_g = tf.expand_dims(g, 0)\n", "\n",
            "            # Append on a 'tower' dimension which we will average over below.\n",
            "            grads.append(expanded_g)\n", "\n",
            "        # Average over the 'tower' dimension.\n",
            "        grad = tf.concat(axis=0, values=grads)\n",
            "        grad = tf.reduce_mean(grad, 0)\n", "\n",
            "        # Keep in mind that the Variables are redundant because they are shared\n",
            "        # across towers. So .. we will just return the first tower's pointer to\n",
            "        # the Variable.\n", "        v = grad_and_vars[0][1]\n",
            "        grad_and_var = (grad, v)\n",
            "        average_grads.append(grad_and_var)\n",
            "    return average_grads"
        ]
    }, {
        "cell_type":
        "code",
        "execution_count":
        3,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Resnet blocks \n",
            "def identity_block(input_tensor, filters, stage, block, is_training):\n",
            "    with tf.variable_scope(\"stage-{}_block-{}\".format(stage, block), reuse=tf.AUTO_REUSE) as scope:\n",
            "        x = tf.layers.conv2d(input_tensor, filters[0], 1, padding=\"same\")\n",
            "        x = tf.layers.batch_normalization(x, training=is_training)\n",
            "        x = tf.nn.relu(x)\n", "\n",
            "        x = tf.layers.conv2d(x, filters[0], 3, padding=\"same\")\n",
            "        x = tf.layers.batch_normalization(x, training=is_training)\n",
            "        x = tf.nn.relu(x)\n", "        \n",
            "        x = tf.layers.conv2d(x, filters[1], 1, padding=\"same\")\n",
            "        x = tf.layers.batch_normalization(x, training=is_training)\n",
            "\n", "        x = tf.math.add(x, input_tensor)\n",
            "        x = tf.nn.relu(x)\n", "    return x\n", "\n",
            "def conv_block(input_tensor, filters, stage, block, is_training, strides=2):\n",
            "    with tf.variable_scope(\"stage-{}_block-{}\".format(stage, block), reuse=tf.AUTO_REUSE) as scope:\n",
            "        x = tf.layers.conv2d(input_tensor, filters[0], 1, strides=strides, padding=\"same\")\n",
            "        x = tf.layers.batch_normalization(x, training=is_training)\n",
            "        x = tf.nn.relu(x)\n", "\n",
            "        x = tf.layers.conv2d(x, filters[0], 3, padding=\"same\")\n",
            "        x = tf.layers.batch_normalization(x, training=is_training)\n",
            "        x = tf.nn.relu(x)\n", "        \n",
            "        x = tf.layers.conv2d(x, filters[1], 1, padding=\"same\")\n",
            "        x = tf.layers.batch_normalization(x, training=is_training)\n",
            "\n",
            "        shortcut = tf.layers.conv2d(input_tensor, filters[1], 1, strides=strides, padding=\"same\")\n",
            "        shortcut = tf.layers.batch_normalization(shortcut, training=is_training)\n",
            "\n", "        x = tf.math.add(x, shortcut)\n",
            "        x = tf.nn.relu(x)        \n", "    return x\n", "\n",
            "# slimmed downed resnet\n",
            "def classifier(inputs, num_classes, num_embeddings, is_training):\n",
            "    with tf.variable_scope(\"stage-1_block-a\", reuse=tf.AUTO_REUSE) as scope:\n",
            "        x = tf.layers.conv2d(inputs, 16, 7, padding=\"same\")\n",
            "        x = tf.layers.batch_normalization(x, training=is_training)\n",
            "        x = tf.nn.relu(x)\n",
            "        x = tf.layers.max_pooling2d(x, 3, strides=2, padding=\"same\")\n",
            "\n",
            "    x = conv_block    (x, [16, 64], stage=2, block='a', is_training=is_training)\n",
            "    x = identity_block(x, [16, 64], stage=2, block='b', is_training=is_training)\n",
            "\n",
            "    x = conv_block    (x, [32, 128], stage=3, block='a', is_training=is_training)\n",
            "    x = identity_block(x, [32, 128], stage=3, block='b', is_training=is_training)\n",
            "\n",
            "    x = conv_block    (x, [64, 128], stage=4, block='a', is_training=is_training)\n",
            "    x = identity_block(x, [64, 128], stage=4, block='b', is_training=is_training)\n",
            "\n",
            "    with tf.variable_scope(\"embeddings\", reuse=tf.AUTO_REUSE) as scope:\n",
            "        x = tf.layers.average_pooling2d(x, (x.get_shape()[-3], x.get_shape()[-2]), 1) #global average pooling\n",
            "        x = tf.layers.flatten(x)\n",
            "        embeddings = tf.layers.dense(x, num_embeddings, activation=None)\n",
            "        \n",
            "    with tf.variable_scope(\"logits\", reuse=tf.AUTO_REUSE) as scope:\n",
            "        pred_logits = tf.nn.relu(embeddings)\n",
            "        pred_logits = tf.layers.dense(pred_logits, num_classes, activation=None)\n",
            "\n", "    return embeddings, pred_logits"
        ]
    }, {
        "cell_type":
        "code",
        "execution_count":
        4,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Center Loss (Metric Learning)\n",
            "# https://github.com/EncodeTS/TensorFlow_Center_Loss/blob/master/center_loss.py\n",
            "def get_center_loss(features, labels, alpha, num_classes):\n",
            "    \n", "    len_features = features.get_shape()[1] \n",
            "    with tf.variable_scope(\"central_loss\", reuse=tf.AUTO_REUSE) as scope:\n",
            "        centers = tf.get_variable('centers', [num_classes, len_features], dtype=tf.float32,\n",
            "            initializer=tf.constant_initializer(0), trainable=False)\n",
            "        \n", "    labels = tf.reshape(labels, [-1])\n", "\n",
            "    centers_batch = tf.gather(centers, labels)\n",
            "    loss = tf.nn.l2_loss(features - centers_batch)\n", "\n",
            "    unique_label, unique_idx, unique_count = tf.unique_with_counts(labels)\n",
            "    appear_times = tf.gather(unique_count, unique_idx)\n",
            "    appear_times = tf.reshape(appear_times, [-1, 1])\n", "\n",
            "    diff = centers_batch - features\n",
            "    diff = diff / tf.cast((1 + appear_times), tf.float32)\n",
            "    diff = alpha * diff\n", "\n",
            "    centers_update_op = tf.scatter_sub(centers, labels, diff)\n",
            "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, centers_update_op)\n",
            "\n", "    return loss, centers"
        ]
    }, {
        "cell_type":
        "code",
        "execution_count":
        5,
        "metadata": {},
        "outputs": [],
        "source": [
            "def unison_shuffled_copies(a, b):\n",
            "    p = np.random.permutation(a.shape[0])\n",
            "    return a[p], b[p]\n", "\n",
            "def get_batch(data_x, data_y, batch_size):\n",
            "    while True:\n",
            "        data_x, data_y = unison_shuffled_copies(data_x, data_y)\n",
            "        for index in range(0, data_x.shape[0], batch_size):\n",
            "            x, y = data_x[index:index+batch_size], data_y[index:index+batch_size]\n",
            "            if x.shape[0] == batch_size:\n",
            "                yield x, y"
        ]
    }, {
        "cell_type":
        "code",
        "execution_count":
        6,
        "metadata": {},
        "outputs": [{
            "name":
            "stdout",
            "output_type":
            "stream",
            "text": [
                "(1360, 2000, 270, 1) (1360, 40) (240, 2000, 270, 1) (240, 40)\n"
            ]
        }],
        "source": [
            "# define model parameters and load dataset\n", "\n",
            "data_file =        \"data/CSI_50_500.h5\"\n",
            "tensorboard_path = \"data/logs/central_loss_50_500/\"\n",
            "weights_path =     \"data/weights/central_loss_50_500/central_loss_model.ckpt\"\n",
            "\n", "num_embeddings = 64\n", "save_step = 25\n",
            "decay_rate = 0.999\n", "batch_size = 16\n", "num_gpus = 4\n",
            "alpha = 0.5\n", "ratio = 0.9\n", "epochs = 1000\n", "lr = 1e-3\n",
            "\n", "hf = h5py.File(data_file, 'r')\n",
            "train_classes = np.array(hf.get('labels')).astype(str)\n",
            "num_classes = len(train_classes)\n",
            "X_train = np.expand_dims(hf.get('X_train'), axis=-1)\n",
            "X_test = np.expand_dims(hf.get('X_test'), axis=-1)\n",
            "y_train = np.eye(num_classes)[hf.get('y_train')]\n",
            "y_test = np.eye(num_classes)[hf.get('y_test')]\n", "hf.close()\n",
            "\n", "train_steps = X_train.shape[0]//(batch_size*num_gpus)\n",
            "test_steps = X_test.shape[0]//(batch_size*num_gpus)\n",
            "train_data = get_batch(X_train, y_train, batch_size*num_gpus)\n",
            "test_data = get_batch(X_test, y_test, batch_size*num_gpus)\n",
            "rows, cols, channels = X_train.shape[1:]\n", "\n",
            "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
            "sys.stdout.flush()"
        ]
    }, {
        "cell_type":
        "code",
        "execution_count":
        7,
        "metadata": {},
        "outputs": [],
        "source": [
            "# method to train model with multilpe gpus \n",
            "tf.reset_default_graph()\n", "\n", "with tf.device('/cpu:0'):\n",
            "    with tf.variable_scope(\"Inputs\") as scope:\n",
            "        X = tf.placeholder(tf.float32, [None, rows, cols, channels])\n",
            "        Y = tf.placeholder(tf.float32, [None, num_classes])\n",
            "        \n", "    is_training = tf.placeholder(tf.bool)\n", "\n",
            "    global_step = tf.train.get_or_create_global_step()\n", "\n",
            "    learning_rate = tf.train.exponential_decay(lr,\n",
            "                                               global_step,\n",
            "                                               train_steps,\n",
            "                                               decay_rate,\n",
            "                                               staircase=True)\n",
            "    tf.summary.scalar(\"learning_rate\", learning_rate)       \n",
            "\n",
            "    opt = tf.train.AdamOptimizer(learning_rate=learning_rate)      \n",
            "\n", "    # Calculate the gradients for each model tower.\n",
            "    tower_grads = []\n", "    center_losses = []\n",
            "    softmax_losses = []\n", "    combined_losses = []\n",
            "    accuracies = []\n", "    for i in range(num_gpus):\n",
            "        with tf.device(\"/gpu:{}\".format(i)):\n",
            "            with tf.name_scope(\"resnet_{}\".format(i)) as scope:\n",
            "                _x = X[i * batch_size: (i+1) * batch_size]\n",
            "                _y = Y[i * batch_size: (i+1) * batch_size]\n",
            "\n",
            "                embeddings, pred_logits = classifier(_x, num_classes, num_embeddings, is_training)\n",
            "\n",
            "                center_loss, centers = get_center_loss(embeddings, tf.argmax(_y, axis=1), alpha, num_classes)\n",
            "                softmax_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=_y, logits=pred_logits))\n",
            "                combined_loss = softmax_loss + ratio * center_loss\n",
            "                grads = opt.compute_gradients(combined_loss) \n",
            "                \n",
            "                accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred_logits, 1), tf.argmax(_y, 1)), tf.float32))\n",
            "\n", "                center_losses.append(center_loss)\n",
            "                softmax_losses.append(softmax_loss)\n",
            "                combined_losses.append(combined_loss)\n",
            "                accuracies.append(accuracy)\n",
            "                tower_grads.append(grads)\n", "\n",
            "    grads = average_gradients(tower_grads)    \n",
            "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
            "    with tf.control_dependencies(update_ops):\n",
            "        apply_gradient_op = opt.apply_gradients(grads, global_step)\n",
            "        \n",
            "    avg_center_loss = tf.reduce_mean(center_losses)\n",
            "    avg_softmax_loss = tf.reduce_mean(softmax_losses)\n",
            "    avg_combined_loss = tf.reduce_mean(combined_losses)\n",
            "    avg_accuracies = tf.reduce_mean(accuracies)\n", "\n",
            "    tf.summary.scalar(\"avg_center_loss\", avg_center_loss)      \n",
            "    tf.summary.scalar(\"avg_softmax_loss\", avg_softmax_loss)      \n",
            "    tf.summary.scalar(\"avg_combined_loss\", avg_combined_loss) \n",
            "    tf.summary.scalar(\"avg_accuracies\", avg_accuracies) \n",
            "    \n", "    # Initializing the variables\n",
            "    init = tf.global_variables_initializer()\n",
            "    merged = tf.summary.merge_all()\n",
            "    saver = tf.train.Saver(max_to_keep=5)"
        ]
    }, {
        "cell_type":
        "code",
        "execution_count":
        null,
        "metadata": {},
        "outputs": [{
            "name": "stdout",
            "output_type": "stream",
            "text": ["Current Epoch: 78\r"]
        }],
        "source": [
            "# train the model, log metrics to tensorboard and save model weights \n",
            "\n",
            "with tf.Session(config=tf.ConfigProto(allow_soft_placement = True)) as sess:\n",
            "    train_writer = tf.summary.FileWriter(tensorboard_path+\"train\", sess.graph)\n",
            "    test_writer = tf.summary.FileWriter(tensorboard_path+\"test\", sess.graph)\n",
            "    sess.run(init)\n", "\n",
            "    for epoch in range(1, epochs + 1):\n",
            "        for step in range(1, train_steps + 1):\n",
            "            batch_x, batch_y = next(train_data)\n",
            "            _, summary, curr_step = sess.run([apply_gradient_op, merged, global_step], feed_dict={X: batch_x, Y: batch_y, is_training: True})\n",
            "            train_writer.add_summary(summary, curr_step)\n",
            "        \n", "        for step in range(1, test_steps+1):\n",
            "            batch_x, batch_y = next(test_data)\n",
            "            summary = sess.run(merged, feed_dict={X: batch_x, Y: batch_y, is_training: False})\n",
            "            test_writer.add_summary(summary, (epoch*test_steps)+step)\n",
            "            \n", "        if (epoch % save_step == 0):\n",
            "            saver.save(sess, weights_path, global_step=curr_step)\n",
            "            \n",
            "        sys.stdout.write(\"Current Epoch: {}\\r\".format(epoch))\n",
            "        sys.stdout.flush()\n", "            \n",
            "    saver.save(sess, weights_path, global_step=curr_step)"
        ]
    }],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.2"
        }
    },
    "nbformat":
    4,
    "nbformat_minor":
    4
}
